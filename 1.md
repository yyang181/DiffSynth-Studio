# DiffSynth Studio
[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/)

<p align="center">
<a href="https://trendshift.io/repositories/10946" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</p>

Document: https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html

## Introduction

Welcome to the magic world of Diffusion models!

DiffSynth consists of two open-source projects:
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): Focused on aggressive technological exploration. Targeted at academia. Provides more cutting-edge technical support and novel inference capabilities.
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.

DiffSynth-Studio is an open-source project aimed at exploring innovations in AIGC technology. We have integrated numerous open-source Diffusion models, including FLUX and Wan, among others. Through this open-source project, we hope to connect models within the open-source community and explore new technologies based on diffusion models.

Until now, DiffSynth-Studio has supported the following models:

* [Wan-Video](https://github.com/Wan-Video/Wan2.1)
* [StepVideo](https://github.com/stepfun-ai/Step-Video-T2V)
* [HunyuanVideo](https://github.com/Tencent/HunyuanVideo), [HunyuanVideo-I2V]()
* [CogVideoX](https://huggingface.co/THUDM/CogVideoX-5b)
* [FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev)
* [ExVideo](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1)
* [Kolors](https://huggingface.co/Kwai-Kolors/Kolors)
* [Stable Diffusion 3](https://huggingface.co/stabilityai/stable-diffusion-3-medium)
* [Stable Video Diffusion](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt)
* [Hunyuan-DiT](https://github.com/Tencent/HunyuanDiT)
* [RIFE](https://github.com/hzwer/ECCV2022-RIFE)
* [ESRGAN](https://github.com/xinntao/ESRGAN)
* [Ip-Adapter](https://github.com/tencent-ailab/IP-Adapter)
* [AnimateDiff](https://github.com/guoyww/animatediff/)
* [ControlNet](https://github.com/lllyasviel/ControlNet)
* [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
* [Stable Diffusion](https://huggingface.co/runwayml/stable-diffusion-v1-5)

## News
- **June 15, 2025** ModelScope's official evaluation framework, [EvalScope](https://github.com/modelscope/evalscope), now supports text-to-image generation evaluation. Try it with the [Best Practices](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html) guide.

- **March 31, 2025** We support InfiniteYou, an identity preserving method for FLUX. Please refer to [./examples/InfiniteYou/](./examples/InfiniteYou/) for more details.

- **March 25, 2025** 🔥🔥🔥 Our new open-source project, [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine), is now open-sourced! Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.

- **March 13, 2025** We support HunyuanVideo-I2V, the image-to-video generation version of HunyuanVideo open-sourced by Tencent. Please refer to [./examples/HunyuanVideo/](./examples/HunyuanVideo/) for more details.

- **February 25, 2025** We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See [./examples/wanvideo/](./examples/wanvideo/).

- **February 17, 2025** We support [StepVideo](https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary)! State-of-the-art video synthesis model! See [./examples/stepvideo](./examples/stepvideo/).

- **December 31, 2024** We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see [./examples/EntityControl](./examples/EntityControl/).
  - Paper: [EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
  - Online Demo: [ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
  - Training Dataset: [EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

- **December 19, 2024** We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to [./examples/HunyuanVideo/](./examples/HunyuanVideo/) for more details.

- **December 18, 2024** We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.
  - Paper: https://arxiv.org/abs/2412.12888
  - Examples: https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
  - Demo: [ModelScope](https://modelscope.cn/aigc/imageGeneration?tab=advanced&versionId=7228&modelType=LoRA&sdVersion=FLUX_1&modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0), HuggingFace (Coming soon)

- **October 25, 2024** We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See [`./examples/ControlNet/`](./examples/ControlNet/).

- **October 8, 2024.** We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) or [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1).

- **August 22, 2024.** CogVideoX-5B is supported in this project. See [here](/examples/video_synthesis/). We provide several interesting features for this text-to-video model, including
  - Text to video
  - Video editing
  - Self-upscaling
  - Video interpolation

- **August 22, 2024.** We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!
  - Use it in our [WebUI](#usage-in-webui).

- **August 21, 2024.** FLUX is supported in DiffSynth-Studio.
  - Enable CFG and highres-fix to improve visual quality. See [here](/examples/image_synthesis/README.md)
  - LoRA, ControlNet, and additional models will be available soon.

- **June 21, 2024.** We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.
  - [Project Page](https://ecnu-cilab.github.io/ExVideoProjectPage/)
  - Source code is released in this repo. See [`examples/ExVideo`](./examples/ExVideo/).
  - Models are released on [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1) and [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1).
  - Technical report is released on [arXiv](https://arxiv.org/abs/2406.14130).
  - You can try ExVideo in this [Demo](https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1)!

- **June 13, 2024.** DiffSynth Studio is transferred to ModelScope. The developers have transitioned from "I" to "we". Of course, I will still participate in development and maintenance.

- **Jan 29, 2024.** We propose Diffutoon, a fantastic solution for toon shading.
  - [Project Page](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
  - The source codes are released in this project.
  - The technical report (IJCAI 2024) is released on [arXiv](https://arxiv.org/abs/2401.16224).

- **Dec 8, 2023.** We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.

- **Nov 15, 2023.** We propose FastBlend, a powerful video deflickering algorithm.
  - The sd-webui extension is released on [GitHub](https://github.com/Artiprocher/sd-webui-fastblend).
  - Demo videos are shown on Bilibili, including three tasks.
    - [Video deflickering](https://www.bilibili.com/video/BV1d94y1W7PE)
    - [Video interpolation](https://www.bilibili.com/video/BV1Lw411m71p)
    - [Image-driven video rendering](https://www.bilibili.com/video/BV1RB4y1Z7LF)
  - The technical report is released on [arXiv](https://arxiv.org/abs/2311.09265).
  - An unofficial ComfyUI extension developed by other users is released on [GitHub](https://github.com/AInseven/ComfyUI-fastblend).

- **Oct 1, 2023.** We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.
  - The source codes are released on [GitHub](https://github.com/Artiprocher/FastSDXL).
  - FastSDXL includes a trainable OLSS scheduler for efficiency improvement.
    - The original repo of OLSS is [here](https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler).
    - The technical report (CIKM 2023) is released on [arXiv](https://arxiv.org/abs/2305.14677).
    - A demo video is shown on [Bilibili](https://www.bilibili.com/video/BV1w8411y7uj).
    - Since OLSS requires additional training, we don't implement it in this project.

- **Aug 29, 2023.** We propose DiffSynth, a video synthesis framework.
  - [Project Page](https://ecnu-cilab.github.io/DiffSynth.github.io/).
  - The source codes are released in [EasyNLP](https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth).
  - The technical report (ECML PKDD 2024) is released on [arXiv](https://arxiv.org/abs/2308.03463).


## Installation

Install from source code (recommended):

```bash
git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio

modelscope download yyang181/envs UniAnimate-DiT-VACE.tar.gz --local_dir /opt/data/private/yyx/envs
mkdir /root/miniconda3/envs/UniAnimate-DiT-VACE
tar -xzf UniAnimate-DiT-VACE.tar.gz -C /root/miniconda3/envs/UniAnimate-DiT-VACE/


conda create --name diffsynth --clone UniAnimate-DiT-VACE
# or from source
conda create -n diffsynth python=3.10
conda activate diffsynth

pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124
conda install nvidia/label/cuda-12.4.1::cuda
$export CUDA_HOME=/root/miniconda3/envs/diffsynth
$export PATH=$CUDA_HOME/bin:$PATH
$export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# cannot find cuda_fp16.h when install sage attention
find $CONDA_PREFIX -name "cuda_fp16.h"     找到了cuda_fp16.h的路径
export CFLAGS="-I/HOME/paratera_xy/pxy545/miniconda3/envs/diffsynth/targets/x86_64-linux/include"
export CXXFLAGS="-I/HOME/paratera_xy/pxy545/miniconda3/envs/diffsynth/targets/x86_64-linux/include"


pip install -e .

### sage attention 
git clone https://githubfast.com/thu-ml/SageAttention.git
cd SageAttention 

# 如果在login节点上执行，export TORCH_CUDA_ARCH_LIST="8.0"  # 根据你目标的 GPU 设定 a100 并且设置 L.80
# if not compute_capabilities:
#     warnings.warn("No suitable GPU found, defaulting to compute capability 8.0")
#     compute_capabilities.add("8.0")

python setup.py install  # or pip install -e .

### basicvsr
# install basicsr for data degradation in STAR training
pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple
pip install opencv-python basicsr ConfigParser openmim pyyaml einops decord
mim install mmcv
# fix package error /HOME/paratera_xy/pxy545/miniconda3/envs/diffsynth/lib/python3.10/site-packages/basicsr/data/degradations.py 
# from torchvision.transforms.functional_tensor import rgb_to_grayscale
from torchvision.transforms.functional import rgb_to_grayscale


pip install -e .

# support swanlab local mode
pip install 'swanlab[dashboard]'

ln -s /opt/data/private/yyx/code/UniAnimate-DiT/models/iic /opt/data/private/yyx/code/DiffSynth-Studio/models/iic/
ln -s /opt/data/private/yyx/code/UniAnimate-DiT/models/Wan2.1-T2V-1.3B /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan2.1-T2V-1.3B/

# toy dataset
modelscope download --dataset DiffSynth-Studio/example_video_dataset --local_dir ./data/example_video_dataset

huggingface-cli download --resume-download Wan-AI/Wan2.1-VACE-1.3B --local-dir /opt/data/private/yyx/code/DiffSynth-Studio/models/HF_Wan2.1-VACE-1.3B

```

Or install from pypi (There is a delay in the update. If you want to experience the latest features, please do not use this installation method.):

```
pip install diffsynth
```

If you encounter issues during installation, it may be caused by the packages we depend on. Please refer to the documentation of the package that caused the problem.

* [torch](https://pytorch.org/get-started/locally/)
* [sentencepiece](https://github.com/google/sentencepiece)
* [cmake](https://cmake.org)
* [cupy](https://docs.cupy.dev/en/stable/install.html)

## SBATCH Training
```bash 
sbatch train.sbatch
sbatch test.sbatch

sbatch --partition=pxy545 --gres=gpu:8 --job-name=sr_vace_datapth train_slurm.sbatch
sbatch --partition=pxy545 --gres=gpu:8 --job-name=sr_vacefull_datapth train_slurm.sbatch
sbatch --partition=pxy545 --gres=gpu:4 --job-name=sr_vacefull_datapth train_slurm.sbatch
sbatch --partition=pxy545 --gres=gpu:4 --job-name=sr_vacefull_datapth_v2 train_slurm.sbatch


# A800 parallel to A800
rsync -av --progress \
  --include '*/' \
  --include 'sr_vace_datapth_8gpu_resume19/***' \
  --exclude '*' \
  -e 'ssh -p 22' \
  pxy545@GUANGZHOUXY-A800PN@ssh.cn-zhongwei-1.paracloud.com:/XYFS01/HOME/paratera_xy/pxy545/code/DiffSynth-Studio/exp/train/ \
  /opt/data/private/yyx/code/DiffSynth-Studio/exp/train

```
## Inference
```bash
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/test.py --checkpoint models/iic/VACE-Wan2.1-1.3B-Preview/diffusion_pytorch_model.safetensors
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/test.py --checkpoint models/HF_Wan2.1-VACE-1.3B/diffusion_pytorch_model.safetensors
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/test.py --checkpoint exp/train/Wan2.1-VACE-1.3B_full/epoch-42.safetensors
CUDA_VISIBLE_DEVICES=4 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/test.py --checkpoint exp/train/Wan2.1-VACE-1.3B_full_4gpu/epoch-144.safetensors
CUDA_VISIBLE_DEVICES=4 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/test_batch.py --checkpoint exp/train/Wan2.1-VACE-1.3B_full_4gpu_datapth
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_test_batch.py --checkpoint exp/train/sr_vace_datapth_4gpu
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_test_batch.py --checkpoint exp/train/sr_Wan2.1-VACE-1.3B_full
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_test_batch.py --checkpoint exp/train/sr_vace_datapth_8gpu_resume19
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_test_batch.py --checkpoint exp/train/sr_vace_datapth_8gpu_resume19_resume58
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_test_batch_winputvideo.py --checkpoint exp/train/sr_vace_datapth_8gpu_resume19_resume58
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_test_batch.py --checkpoint exp/train/sr_vacefull
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_8gpu_parallel
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_8gpu_bs16
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_8gpu_bs16 --winputvideo
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_4gpu_bs16_resume50 --winputvideo --cfg_scale 1.0
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_v2_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_v2_4gpu_bs16
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_v2_test_batch.py --checkpoint exp/train/sr_vacefull_datapth_v2_4gpu_bs16 --cfg_scale 1.0
CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_v2_test_batch_realtrain.py --checkpoint exp/train/sr_vacefull_datapth_v2_4gpu_bs16 --cfg_scale 1.0
CUDA_VISIBLE_DEVICES=1 torchrun --standalone --nproc_per_node=1 examples/wanvideo/model_inference/sr_vacefull_v2_test_batch_realtrain.py --checkpoint exp/train/sr_vacefull_v2_bs12_openvidhd_wnegprompt --cfg_scale 5.0


```

## VACE Training 
```bash
### vace original 
CUDA_VISIBLE_DEVICES=0,1,2,3 \
accelerate launch --mixed_precision=bf16 \
  --main_process_port 20000 \
  --num_processes 4 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train.py \
  --dataset_base_path data/example_video_dataset \
  --dataset_metadata_path data/example_video_dataset/metadata_vace.csv \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 100 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-VACE-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --output_path "./exp/train/Wan2.1-VACE-1.3B_full_4gpu" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode cloud 

### vace original with data pt
CUDA_VISIBLE_DEVICES=0,1,2,3 \
accelerate launch --mixed_precision=bf16 \
  --main_process_port 20000 \
  --num_processes 4 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train.py \
  --dataset_base_path data/example_video_dataset \
  --dataset_metadata_path data/example_video_dataset/metadata_vace.csv \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 100 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-VACE-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --resume_from_checkpoint ./exp/train/Wan2.1-VACE-1.3B_full_4gpu/epoch-189.safetensors \
  --output_path "./exp/train/Wan2.1-VACE-1.3B_full_4gpu_datapth" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode cloud --use_data_pt exp/train/data_process

### test only 
# upgrade accelerate==1.6.0 to 1.8.1
# pip install accelerate==1.8.1
CUDA_VISIBLE_DEVICES=4,5,6,7 \
accelerate launch --mixed_precision=bf16 \
  --num_processes 4 \
  --main_process_port 20004 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train.py \
  --dataset_base_path data/example_video_dataset \
  --dataset_metadata_path data/example_video_dataset/metadata_vace.csv \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 100 \
  --model_id_with_origin_paths "iic/VACE-Wan2.1-1.3B-Preview:diffusion_pytorch_model*.safetensors,iic/VACE-Wan2.1-1.3B-Preview:models_t5_umt5-xxl-enc-bf16.pth,iic/VACE-Wan2.1-1.3B-Preview:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --output_path "./exp/testonly" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode cloud

# TODO
# Update Swanlab accelerate official support https://docs.swanlab.cn/guide_cloud/integration/integration-huggingface-accelerate.html

### data preprocess
CUDA_VISIBLE_DEVICES=4 python examples/wanvideo/model_training/train.py \
  --dataset_base_path data/example_video_dataset \
  --dataset_metadata_path data/example_video_dataset/metadata_vace.csv \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 100 \
  --model_id_with_origin_paths "iic/VACE-Wan2.1-1.3B-Preview:diffusion_pytorch_model*.safetensors,iic/VACE-Wan2.1-1.3B-Preview:models_t5_umt5-xxl-enc-bf16.pth,iic/VACE-Wan2.1-1.3B-Preview:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --output_path "./exp/train/data_process" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --data_process
```

## SR VACE Training v1
```bash 
### sr vace 
# cp models/iic/VACE-Wan2.1-1.3B-Preview/diffusion_pytorch_model.safetensors models/Wan-AI/Wan2.1-VACE-1.3B
# cp models/iic/VACE-Wan2.1-1.3B-Preview/models_t5_umt5-xxl-enc-bf16.pth models/Wan-AI/Wan2.1-VACE-1.3B
# cp models/iic/VACE-Wan2.1-1.3B-Preview/Wan2.1_VAE.pth models/Wan-AI/Wan2.1-VACE-1.3B

# ln must use absolute path 
# ln -s /opt/data/private/yyx/code/DiffSynth-Studio/models/iic/VACE-Wan2.1-1.3B-Preview/diffusion_pytorch_model.safetensors /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan-AI/Wan2.1-VACE-1.3B/
# ln -s /opt/data/private/yyx/code/DiffSynth-Studio/models/iic/VACE-Wan2.1-1.3B-Preview/models_t5_umt5-xxl-enc-bf16.pth /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan-AI/Wan2.1-VACE-1.3B
# ln -s /opt/data/private/yyx/code/DiffSynth-Studio/models/iic/VACE-Wan2.1-1.3B-Preview/Wan2.1_VAE.pth /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan-AI/Wan2.1-VACE-1.3B

CUDA_VISIBLE_DEVICES=4 \
accelerate launch --mixed_precision=bf16 \
  --num_processes 1 \
  --main_process_port 20003 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 2 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-VACE-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --output_path "./exp/train/sr_vace" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --is_sr \
  --use_data_pt exp/train/data_process/data_cache

### sr vacefull with data pt
# check model hash:
python check_model_config_hash.py /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors  

CUDA_VISIBLE_DEVICES=0,1 \
accelerate launch --mixed_precision=bf16 \
  --num_processes 2 \
  --main_process_port 20003 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train_v1_vacefull.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --batch_size 16 \
  --dataset_repeat 4 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 5e-5 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.dit." \
  --resume_from_checkpoint ./exp/train/sr_vacefull_bs16/epoch-7.safetensors \
  --output_path "./exp/train/sr_vacefull_bs16_resume7" \
  --trainable_models "dit" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --is_sr \
  --use_data_pt ../../data/OpenVidHD/train_pth_v2

### vacefull data preprocess
# decode pth 
CUDA_VISIBLE_DEVICES=1 python examples/wanvideo/model_inference/decode_pth.py
# encode pth
CUDA_VISIBLE_DEVICES=0,1 python examples/wanvideo/model_training/train_v1_vacefull.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 2 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.dit." \
  --output_path "/opt/data/private/yyx/data/OpenVidHD/train_pth_v2" \
  --trainable_models "dit" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --is_sr --data_process

### data preprocess
CUDA_VISIBLE_DEVICES=4 python examples/wanvideo/model_training/train.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 2 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-VACE-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.vace." \
  --output_path "/opt/data/private/yyx/data/OpenVidHD/train_pth" \
  --trainable_models "vace" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --is_sr --data_process

```

## SR VACE Training v2
```bash 
### sr vacefull with data pt
# check model hash:
python check_model_config_hash.py /opt/data/private/yyx/code/DiffSynth-Studio/models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors  

CUDA_VISIBLE_DEVICES=1 \
accelerate launch --mixed_precision=bf16 \
  --num_processes 1 \
  --main_process_port 20003 \
  --config_file ./examples/wanvideo/model_training/full/accelerate_config_1point3B.yaml\
  examples/wanvideo/model_training/train_v2_vacefull.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --batch_size 16 \
  --dataset_repeat 1 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 5e-5 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.dit." \
  --output_path "./exp/train/sr_vacefull_v2_bs12_openvidhd_wnegprompt" \
  --trainable_models "dit" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing --use_swanlab --swanlab_mode cloud --is_sr \
  --use_data_pt ../../data/OpenVidHD_cjy/Video_Musiq_Maniqa_sort_level_I_Tensor_49_frames

  --use_gradient_checkpointing_offload
  --use_data_pt ../../data/OpenVidHD/train_pth_v2
  --resume_from_checkpoint ./exp/train/sr_vacefull_bs16/epoch-7.safetensors \

### vacefull data preprocess
# decode pth 
CUDA_VISIBLE_DEVICES=1 python examples/wanvideo/model_inference/decode_pth.py
# encode pth
CUDA_VISIBLE_DEVICES=0,1 python examples/wanvideo/model_training/train_v1_vacefull.py \
  --dataset_base_path ../../data/OpenVidHD \
  --dataset_metadata_path ../../data/OpenVidHD/data_csv/OpenVidHD.csv \
  --degradation_config_path config/degradations.yaml \
  --data_file_keys "video,vace_video,vace_reference_image" \
  --height 480 \
  --width 832 \
  --num_frames 49 \
  --dataset_repeat 2 \
  --model_id_with_origin_paths "Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors,Wan-AI/Wan2.1-VACE-1.3B:models_t5_umt5-xxl-enc-bf16.pth,Wan-AI/Wan2.1-VACE-1.3B:Wan2.1_VAE.pth" \
  --learning_rate 1e-4 \
  --num_epochs 1000000000000000 \
  --remove_prefix_in_ckpt "pipe.dit." \
  --output_path "/opt/data/private/yyx/data/OpenVidHD/train_pth_v2" \
  --trainable_models "dit" \
  --extra_inputs "vace_video,vace_reference_image,degradation_kernels,degradation_params" \
  --use_gradient_checkpointing_offload --use_swanlab --swanlab_mode disabled --is_sr --data_process

```

## Usage (in Python code)

The Python examples are in [`examples`](./examples/). We provide an overview here.

### Download Models

Download the pre-set models. Model IDs can be found in [config file](/diffsynth/configs/model_config.py).

```python
from diffsynth import download_models

download_models(["FLUX.1-dev", "Kolors"])
```

Download your own models.

```python
from diffsynth.models.downloader import download_from_huggingface, download_from_modelscope

# From Modelscope (recommended)
download_from_modelscope("Kwai-Kolors/Kolors", "vae/diffusion_pytorch_model.fp16.bin", "models/kolors/Kolors/vae")
# From Huggingface
download_from_huggingface("Kwai-Kolors/Kolors", "vae/diffusion_pytorch_model.fp16.safetensors", "models/kolors/Kolors/vae")
```

### Video Synthesis

#### Text-to-video using CogVideoX-5B

CogVideoX-5B is released by ZhiPu. We provide an improved pipeline, supporting text-to-video, video editing, self-upscaling and video interpolation. [`examples/video_synthesis`](./examples/video_synthesis/)

The video on the left is generated using the original text-to-video pipeline, while the video on the right is the result after editing and frame interpolation.

https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006

#### Long Video Synthesis

We trained extended video synthesis models, which can generate 128 frames. [`examples/ExVideo`](./examples/ExVideo/)

https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc

https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e

#### Toon Shading

Render realistic videos in a flatten style and enable video editing features. [`examples/Diffutoon`](./examples/Diffutoon/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c

#### Video Stylization

Video stylization without video models. [`examples/diffsynth`](./examples/diffsynth/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea

### Image Synthesis

Generate high-resolution images, by breaking the limitation of diffusion models! [`examples/image_synthesis`](./examples/image_synthesis/).

LoRA fine-tuning is supported in [`examples/train`](./examples/train/).

|FLUX|Stable Diffusion 3|
|-|-|
|![image_1024_cfg](https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098)|

|Kolors|Hunyuan-DiT|
|-|-|
|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5)|

|Stable Diffusion|Stable Diffusion XL|
|-|-|
|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5)|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90)|

## Usage (in WebUI)

Create stunning images using the painter, with assistance from AI!

https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0

**This video is not rendered in real-time.**

Before launching the WebUI, please download models to the folder `./models`. See [here](#download-models).

* `Gradio` version

```
pip install gradio
```

```
python apps/gradio/DiffSynth_Studio.py
```

![20240822102002](https://github.com/user-attachments/assets/59613157-de51-4109-99b3-97cbffd88076)

* `Streamlit` version

```
pip install streamlit streamlit-drawable-canvas
```

```
python -m streamlit run apps/streamlit/DiffSynth_Studio.py
```

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954
